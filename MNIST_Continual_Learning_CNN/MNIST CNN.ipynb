{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN transfer learning on MNIST dataset\n",
    "\n",
    "Assignment instructions:\n",
    "1. Create NN and train on MNIST digits 0-4\n",
    "2. Test the NN on 0-9 digits test set\n",
    "3. Apply transfer learning by freezing layer/adding new layers and training on digits 5-9\n",
    "4. Test again on 0-9 digits test set\n",
    "\n",
    "In this notebook I've implemented two different methods for applying the transfer learning: \n",
    "- The first approach uses a sequential model CNN where no layers are added after training on 0-4. The trained weights are copied over and the convolution layers which extract the image features are frozen. The model is then trained on 5-9 using a  slower learning rate (using the SGD optimizer) to reach a saddle point where the model can predict both digit subsets with 80+% accuracy overall.\n",
    "- The second approach uses a branching CNN model where 0-4 is trained on a singular branch model (similar to the first approach model) and the weights are then loaded into a new model that branchs after the convolution layer. The convolution layer and 0-4 digit branch are then frozen and the 5-9 branch is trained resulting in a model with two output layers. Due to there being two output layers each producing their own probabilities on each image, the accuracy of the model is manually calculated by extracting the digit with highest probability of both output layers. The results are 90+% accurate predicitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set network parameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "img_rows = 28\n",
    "img_cols = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and split dataset into test/train\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# split subsets into 0-4 and 5-9 sets (using _lt for less than 5 set, and _gt for greater than 4 set)\n",
    "X_train_lt = X_train[y_train < 5]\n",
    "y_train_lt = y_train[y_train < 5]\n",
    "X_test_lt = X_test[y_test < 5]\n",
    "y_test_lt = y_test[y_test < 5]\n",
    "\n",
    "X_train_gt = X_train[y_train > 4]\n",
    "y_train_gt = y_train[y_train > 4]\n",
    "X_test_gt = X_test[y_test > 4]\n",
    "y_test_gt = y_test[y_test > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data to fit the model\n",
    "X_train_lt = X_train_lt.reshape(X_train_lt.shape[0], img_rows, img_cols, 1)\n",
    "X_test_lt = X_test_lt.reshape(X_test_lt.shape[0], img_rows, img_cols, 1)\n",
    "X_train_lt = X_train_lt.astype('float32')/255\n",
    "X_test_lt = X_test_lt.astype('float32')/255\n",
    "\n",
    "X_train_gt = X_train_gt.reshape(X_train_gt.shape[0], img_rows, img_cols, 1)\n",
    "X_test_gt = X_test_gt.reshape(X_test_gt.shape[0], img_rows, img_cols, 1)\n",
    "X_train_gt = X_train_gt.astype('float32')/255\n",
    "X_test_gt = X_test_gt.astype('float32')/255\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary matrices\n",
    "y_train_lt = keras.utils.to_categorical(y_train_lt, num_classes)\n",
    "y_test_lt = keras.utils.to_categorical(y_test_lt, num_classes)\n",
    "\n",
    "y_train_gt = keras.utils.to_categorical(y_train_gt, num_classes)\n",
    "y_test_gt = keras.utils.to_categorical(y_test_gt, num_classes)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training on 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cnn parameters\n",
    "filters = 32\n",
    "kernel_size = (3, 3)\n",
    "pool_size = (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0904 11:26:59.603841  5876 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0904 11:27:00.112296  5876 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0904 11:27:00.185362  5876 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0904 11:27:00.322502  5876 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0904 11:27:00.362531  5876 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0904 11:27:00.369528  5876 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,630,346\n",
      "Trainable params: 1,630,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create initial model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size, activation='relu', kernel_initializer='he_normal', input_shape=(img_rows, img_cols, 1), \n",
    "                 padding='same'))\n",
    "model.add(Conv2D(32, kernel_size, activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "model.add(Conv2D(64, kernel_size, activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax', kernel_initializer='he_normal'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0904 11:27:04.408509  5876 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0904 11:27:04.416516  5876 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0904 11:27:04.909965  5876 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22947 samples, validate on 7649 samples\n",
      "Epoch 1/10\n",
      "22947/22947 [==============================] - 22s 958us/step - loss: 0.1479 - acc: 0.9567 - val_loss: 0.0272 - val_acc: 0.9912\n",
      "Epoch 2/10\n",
      "22947/22947 [==============================] - 3s 136us/step - loss: 0.0369 - acc: 0.9890 - val_loss: 0.0193 - val_acc: 0.9933\n",
      "Epoch 3/10\n",
      "22947/22947 [==============================] - 3s 137us/step - loss: 0.0233 - acc: 0.9929 - val_loss: 0.0201 - val_acc: 0.9937\n",
      "Epoch 4/10\n",
      "22947/22947 [==============================] - 3s 135us/step - loss: 0.0189 - acc: 0.9940 - val_loss: 0.0186 - val_acc: 0.9946\n",
      "Epoch 5/10\n",
      "22947/22947 [==============================] - 3s 135us/step - loss: 0.0148 - acc: 0.9953 - val_loss: 0.0209 - val_acc: 0.9945\n",
      "Epoch 6/10\n",
      "22947/22947 [==============================] - 3s 136us/step - loss: 0.0104 - acc: 0.9968 - val_loss: 0.0157 - val_acc: 0.9946\n",
      "Epoch 7/10\n",
      "22947/22947 [==============================] - 3s 136us/step - loss: 0.0105 - acc: 0.9968 - val_loss: 0.0159 - val_acc: 0.9957\n",
      "Epoch 8/10\n",
      "22947/22947 [==============================] - 3s 136us/step - loss: 0.0085 - acc: 0.9973 - val_loss: 0.0175 - val_acc: 0.9961\n",
      "Epoch 9/10\n",
      "22947/22947 [==============================] - 3s 138us/step - loss: 0.0063 - acc: 0.9978 - val_loss: 0.0177 - val_acc: 0.9961\n",
      "Epoch 10/10\n",
      "22947/22947 [==============================] - 3s 138us/step - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0177 - val_acc: 0.9963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1447f0bca48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=Adam(), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# train the model\n",
    "model.fit(\n",
    "    X_train_lt, \n",
    "    y_train_lt, \n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    validation_split=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with weights to file\n",
    "model.save('model_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Testing on 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5139/5139 [==============================] - 1s 105us/step\n",
      "Test loss: 0.0040327511951030095\n",
      "Test accuracy: 0.9986378672893559\n"
     ]
    }
   ],
   "source": [
    "# test accuracy on 0-4 test subset\n",
    "score = model.evaluate(X_test_lt, y_test_lt, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4861/4861 [==============================] - 0s 81us/step\n",
      "Test loss: 15.211717275601066\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# test accuracy on 5-9 test subset\n",
    "score = model.evaluate(X_test_gt, y_test_gt, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 79us/step\n",
      "Test loss: 7.396488146209717\n",
      "Test accuracy: 0.5132\n"
     ]
    }
   ],
   "source": [
    "# test accuracy on full 0-9 test set\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training on 5-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open model from file\n",
    "new_model = load_model('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_1 False\n",
      "conv2d_2 False\n",
      "conv2d_3 False\n",
      "max_pooling2d_1 False\n",
      "flatten_1 False\n",
      "dense_1 True\n",
      "dropout_1 True\n",
      "dense_2 True\n"
     ]
    }
   ],
   "source": [
    "# freeze convolution layers and verify\n",
    "for i in range(5):\n",
    "    new_model.layers[i].trainable = False\n",
    "for layer in new_model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set regularization for trainable dense layers\n",
    "new_model.layers[-1].kernel_regularizer=regularizers.l2(0.001)\n",
    "new_model.layers[-3].kernel_regularizer=regularizers.l2(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23523 samples, validate on 5881 samples\n",
      "Epoch 1/10\n",
      "23523/23523 [==============================] - 2s 88us/step - loss: 14.5700 - acc: 0.0000e+00 - val_loss: 14.6008 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "23523/23523 [==============================] - 2s 68us/step - loss: 13.9627 - acc: 8.5023e-05 - val_loss: 13.8172 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "23523/23523 [==============================] - 2s 68us/step - loss: 13.0069 - acc: 2.5507e-04 - val_loss: 12.5501 - val_acc: 1.7004e-04\n",
      "Epoch 4/10\n",
      "23523/23523 [==============================] - 2s 67us/step - loss: 11.5072 - acc: 0.0031 - val_loss: 10.6529 - val_acc: 0.0010\n",
      "Epoch 5/10\n",
      "23523/23523 [==============================] - 2s 67us/step - loss: 9.6085 - acc: 0.0268 - val_loss: 8.4958 - val_acc: 0.0315\n",
      "Epoch 6/10\n",
      "23523/23523 [==============================] - 2s 67us/step - loss: 7.6299 - acc: 0.1029 - val_loss: 6.5371 - val_acc: 0.1755\n",
      "Epoch 7/10\n",
      "23523/23523 [==============================] - 2s 67us/step - loss: 6.0119 - acc: 0.1911 - val_loss: 4.9145 - val_acc: 0.3093\n",
      "Epoch 8/10\n",
      "23523/23523 [==============================] - 2s 67us/step - loss: 4.8277 - acc: 0.2660 - val_loss: 3.7248 - val_acc: 0.4004\n",
      "Epoch 9/10\n",
      "23523/23523 [==============================] - 2s 67us/step - loss: 3.9353 - acc: 0.3306 - val_loss: 2.9525 - val_acc: 0.4991\n",
      "Epoch 10/10\n",
      "23523/23523 [==============================] - 2s 67us/step - loss: 3.3492 - acc: 0.3836 - val_loss: 2.4223 - val_acc: 0.5604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1440592e648>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model with SGD slow rate learning\n",
    "new_model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=SGD(lr=7e-5, momentum=0.5), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# train the model on 5-9\n",
    "new_model.fit(\n",
    "    X_train_gt, \n",
    "    y_train_gt, \n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    validation_split=0.2\n",
    "    #validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing on 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5139/5139 [==============================] - 0s 76us/step\n",
      "Test loss: 0.07988599122968205\n",
      "Test accuracy: 0.9694493092041253\n"
     ]
    }
   ],
   "source": [
    "# test accuracy on 0-4 test subset\n",
    "score = new_model.evaluate(X_test_lt, y_test_lt, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4861/4861 [==============================] - 0s 76us/step\n",
      "Test loss: 2.3018385858796697\n",
      "Test accuracy: 0.5589384899858436\n"
     ]
    }
   ],
   "source": [
    "# test accuracy on 5-9 test subset\n",
    "score = new_model.evaluate(X_test_gt, y_test_gt, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 79us/step\n",
      "Test loss: 1.1599771546840667\n",
      "Test accuracy: 0.7699\n"
     ]
    }
   ],
   "source": [
    "# test accuracy on full 0-9 test set\n",
    "score = new_model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from comparing the test after the 0-4 training and 5-9 training that the model shows large improvement of overall prediction accuracy. The 0-4 predicition accuracy fall 3% after retraining but the 5-9 prediction goes from 0 to 70% giving a 0-9 boost from 51% to 83%.\n",
    "\n",
    "This sequential transfer learned model performs well however when prototyping for optimal network parameters it was apparent that this model has a severe tradeoff between the accuracy of the two subsets resulting in model either being underfit for the new data or forgetting the old data through an update of weight values.\n",
    "\n",
    "Adding new layers after transfer was considered but every implementation of it resulted in catastrophic forgetting of the 0-4 data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branching Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set network parameters\n",
    "batch_size = 128\n",
    "num_classes = 5\n",
    "total_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "img_rows = 28\n",
    "img_cols = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and split dataset into test/train\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# sort into 0-4 and 5-9 sets (using _lt for less than 5 set, and _gt for greater than 4 set)\n",
    "X_train_lt = X_train[y_train < 5]\n",
    "y_train_lt = y_train[y_train < 5]\n",
    "X_test_lt = X_test[y_test < 5]\n",
    "y_test_lt = y_test[y_test < 5]\n",
    "\n",
    "X_train_gt = X_train[y_train > 4]\n",
    "y_train_gt = y_train[y_train > 4] - 5\n",
    "X_test_gt = X_test[y_test > 4]\n",
    "y_test_gt = y_test[y_test > 4] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data to fit the model\n",
    "X_train_lt = X_train_lt.reshape(X_train_lt.shape[0], img_rows, img_cols, 1)\n",
    "X_test_lt = X_test_lt.reshape(X_test_lt.shape[0], img_rows, img_cols, 1)\n",
    "X_train_lt = X_train_lt.astype('float32')/255\n",
    "X_test_lt = X_test_lt.astype('float32')/255\n",
    "\n",
    "X_train_gt = X_train_gt.reshape(X_train_gt.shape[0], img_rows, img_cols, 1)\n",
    "X_test_gt = X_test_gt.reshape(X_test_gt.shape[0], img_rows, img_cols, 1)\n",
    "X_train_gt = X_train_gt.astype('float32')/255\n",
    "X_test_gt = X_test_gt.astype('float32')/255\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary matrices\n",
    "y_train_lt = keras.utils.to_categorical(y_train_lt, num_classes)\n",
    "y_test_lt = keras.utils.to_categorical(y_test_lt, num_classes)\n",
    "\n",
    "y_train_gt = keras.utils.to_categorical(y_train_gt, num_classes)\n",
    "y_test_gt = keras.utils.to_categorical(y_test_gt, num_classes)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, total_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, total_classes)\n",
    "\n",
    "# split labels into corresponding 0-4 and 5-9 sets (including zeros) for dual outputs\n",
    "y_train_lt_dual = y_train[:,:5]\n",
    "y_test_lt_dual = y_test[:,:5]\n",
    "y_train_gt_dual = y_train[:,5:]\n",
    "y_test_gt_dual = y_test[:,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_lt:  (30596, 5)\n",
      "Shape of y_train_gt:  (29404, 5)\n",
      "Shape of y_test_lt:  (5139, 5)\n",
      "Shape of y_test_gt:  (4861, 5)\n"
     ]
    }
   ],
   "source": [
    "# get shape of lt and gt subsets\n",
    "print('Shape of y_train_lt: ', y_train_lt.shape)\n",
    "print('Shape of y_train_gt: ', y_train_gt.shape)\n",
    "print('Shape of y_test_lt: ', y_test_lt.shape)\n",
    "print('Shape of y_test_gt: ', y_test_gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lt and gt shaped zero array for testing an evaluation\n",
    "y_train_gt_zeros = np.zeros([30596,5])\n",
    "y_train_lt_zeros = np.zeros([29404,5])\n",
    "y_test_gt_zeros = np.zeros([5139,5])\n",
    "y_test_lt_zeros = np.zeros([4861,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training on 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cnn parameters\n",
    "filters = 32\n",
    "kernel_size = (3, 3)\n",
    "pool_size = (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "branch_1_dense_1 (Dense)     (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "branch_1_dense_2 (Dense)     (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "branch_1_dropout_1 (Dropout) (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "branch_1_dense_3 (Dense)     (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,646,213\n",
      "Trainable params: 1,646,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create initial model\n",
    "inp = Input(shape=(img_rows, img_cols, 1), name='input')\n",
    "x = Conv2D(16, kernel_size, activation='relu', kernel_initializer='he_normal', padding='same', name='conv2d_1')(inp)\n",
    "x = Conv2D(32, kernel_size, activation='relu', kernel_initializer='he_normal', padding='same', name='conv2d_2')(x)\n",
    "x = Conv2D(64, kernel_size, activation='relu', kernel_initializer='he_normal', padding='same', name='conv2d_3')(x)\n",
    "x = MaxPooling2D(pool_size=pool_size, name='max_pooling2d_1')(x)\n",
    "x = Flatten(name='flatten_1')(x)\n",
    "x = Dense(128, activation='elu', kernel_initializer='he_normal', name='branch_1_dense_1')(x)\n",
    "x = Dense(128, activation='elu', kernel_initializer='he_normal', name='branch_1_dense_2')(x)\n",
    "x = Dropout(0.5, name='branch_1_dropout_1')(x)\n",
    "x = Dense(num_classes, activation='softmax', kernel_initializer='he_normal', name='branch_1_dense_3')(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22947 samples, validate on 7649 samples\n",
      "Epoch 1/10\n",
      "22947/22947 [==============================] - 4s 174us/step - loss: 0.1539 - acc: 0.9528 - val_loss: 0.0276 - val_acc: 0.9922\n",
      "Epoch 2/10\n",
      "22947/22947 [==============================] - 3s 140us/step - loss: 0.0263 - acc: 0.9915 - val_loss: 0.0211 - val_acc: 0.9944\n",
      "Epoch 3/10\n",
      "22947/22947 [==============================] - 3s 140us/step - loss: 0.0137 - acc: 0.9957 - val_loss: 0.0196 - val_acc: 0.9946\n",
      "Epoch 4/10\n",
      "22947/22947 [==============================] - 3s 140us/step - loss: 0.0085 - acc: 0.9973 - val_loss: 0.0194 - val_acc: 0.9952\n",
      "Epoch 5/10\n",
      "22947/22947 [==============================] - 3s 140us/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0240 - val_acc: 0.9937\n",
      "Epoch 6/10\n",
      "22947/22947 [==============================] - 3s 140us/step - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0199 - val_acc: 0.9954\n",
      "Epoch 7/10\n",
      "22947/22947 [==============================] - 3s 140us/step - loss: 0.0045 - acc: 0.9983 - val_loss: 0.0208 - val_acc: 0.9953\n",
      "Epoch 8/10\n",
      "22947/22947 [==============================] - 3s 140us/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0259 - val_acc: 0.9949\n",
      "Epoch 9/10\n",
      "22947/22947 [==============================] - 3s 140us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0234 - val_acc: 0.9945\n",
      "Epoch 10/10\n",
      "22947/22947 [==============================] - 3s 140us/step - loss: 0.0056 - acc: 0.9983 - val_loss: 0.0253 - val_acc: 0.9925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14405937088>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=Adam(), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# train the model\n",
    "model.fit(\n",
    "    X_train_lt, \n",
    "    y_train_lt, \n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    validation_split=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with weights to file\n",
    "model.save('model_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Testing on 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5139/5139 [==============================] - 0s 82us/step\n",
      "Test loss: 0.013370477953510325\n",
      "Test accuracy: 0.9961081922553026\n"
     ]
    }
   ],
   "source": [
    "# test accuracy on 0-4 test subset\n",
    "score = model.evaluate(X_test_lt, y_test_lt, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4861/4861 [==============================] - 0s 81us/step\n",
      "Test loss: 7.151638581904333\n",
      "Test accuracy: 0.2592059246792614\n"
     ]
    }
   ],
   "source": [
    "# test accuracy on 5-9 test subset\n",
    "score = model.evaluate(X_test_gt, y_test_gt, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: due to the model only inputting and outputting half the digits at a time, we'll assume an overall accuracy of the model to be an average of its performance on each subset. Thus 0-9 test accuracy: 0.5996"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training on 5-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 16)   160         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 32)   4640        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 64)   18496       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 64)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 12544)        0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "branch_1_dense_1 (Dense)        (None, 128)          1605760     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "branch_2_dense_1 (Dense)        (None, 128)          1605760     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "branch_1_dense_2 (Dense)        (None, 128)          16512       branch_1_dense_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "branch_2_dense_2 (Dense)        (None, 128)          16512       branch_2_dense_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "branch_1_dropout_1 (Dropout)    (None, 128)          0           branch_1_dense_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "branch_2_dropout_1 (Dropout)    (None, 128)          0           branch_2_dense_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "branch_1_dense_3 (Dense)        (None, 5)            645         branch_1_dropout_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "branch_2_dense_3 (Dense)        (None, 5)            645         branch_2_dropout_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 3,269,130\n",
      "Trainable params: 3,269,130\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# recreate initial model\n",
    "inp = Input(shape=(img_rows, img_cols, 1), name='input')\n",
    "x = Conv2D(16, kernel_size, activation='relu', kernel_initializer='he_normal', padding='same', name='conv2d_1')(inp)\n",
    "x = Conv2D(32, kernel_size, activation='relu', kernel_initializer='he_normal', padding='same', name='conv2d_2')(x)\n",
    "x = Conv2D(64, kernel_size, activation='relu', kernel_initializer='he_normal', padding='same', name='conv2d_3')(x)\n",
    "x = MaxPooling2D(pool_size=pool_size, name='max_pooling2d_1')(x)\n",
    "x = Flatten(name='flatten_1')(x)\n",
    "\n",
    "# output branch 1\n",
    "y1 = Dense(128, activation='elu', kernel_initializer='he_normal', name='branch_1_dense_1')(x)\n",
    "y1 = Dense(128, activation='elu', kernel_initializer='he_normal', name='branch_1_dense_2')(y1)\n",
    "y1 = Dropout(0.5, name='branch_1_dropout_1')(y1)\n",
    "y1 = Dense(num_classes, activation='softmax', kernel_initializer='he_normal', name='branch_1_dense_3')(y1)\n",
    "\n",
    "# output branch 2\n",
    "y2 = Dense(128, activation='elu', kernel_initializer='he_normal', name='branch_2_dense_1')(x)\n",
    "y2 = Dense(128, activation='elu', kernel_initializer='he_normal', name='branch_2_dense_2')(y2)\n",
    "y2 = Dropout(0.5, name='branch_2_dropout_1')(y2)\n",
    "y2 = Dense(num_classes, activation='softmax', kernel_initializer='he_normal', name='branch_2_dense_3')(y2)\n",
    "\n",
    "new_model = Model(inputs=inp, outputs=[y1, y2])\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "branch_1_dense_1 (Dense)     (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "branch_1_dense_2 (Dense)     (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "branch_1_dropout_1 (Dropout) (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "branch_1_dense_3 (Dense)     (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,646,213\n",
      "Trainable params: 1,646,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "old_model = load_model('model_2.h5')\n",
    "old_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,new_layer in enumerate(new_model.layers):\n",
    "    for j,old_layer in enumerate(old_model.layers):\n",
    "        if new_layer.name == old_layer.name:\n",
    "            new_layer.set_weights(old_layer.get_weights())\n",
    "            new_layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input False\n",
      "1 conv2d_1 False\n",
      "2 conv2d_2 False\n",
      "3 conv2d_3 False\n",
      "4 max_pooling2d_1 False\n",
      "5 flatten_1 False\n",
      "6 branch_1_dense_1 False\n",
      "7 branch_2_dense_1 True\n",
      "8 branch_1_dense_2 False\n",
      "9 branch_2_dense_2 True\n",
      "10 branch_1_dropout_1 False\n",
      "11 branch_2_dropout_1 True\n",
      "12 branch_1_dense_3 False\n",
      "13 branch_2_dense_3 True\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(new_model.layers):\n",
    "    print(i,layer.name,layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22053 samples, validate on 7351 samples\n",
      "Epoch 1/10\n",
      "22053/22053 [==============================] - 3s 132us/step - loss: 0.0932 - branch_1_dense_3_loss: 0.0000e+00 - branch_2_dense_3_loss: 0.0932 - branch_1_dense_3_acc: 0.2106 - branch_2_dense_3_acc: 0.9688 - val_loss: 0.0489 - val_branch_1_dense_3_loss: 0.0000e+00 - val_branch_2_dense_3_loss: 0.0489 - val_branch_1_dense_3_acc: 0.2020 - val_branch_2_dense_3_acc: 0.9854\n",
      "Epoch 2/10\n",
      "22053/22053 [==============================] - 2s 95us/step - loss: 0.0247 - branch_1_dense_3_loss: 0.0000e+00 - branch_2_dense_3_loss: 0.0247 - branch_1_dense_3_acc: 0.2077 - branch_2_dense_3_acc: 0.9919 - val_loss: 0.0467 - val_branch_1_dense_3_loss: 0.0000e+00 - val_branch_2_dense_3_loss: 0.0467 - val_branch_1_dense_3_acc: 0.2020 - val_branch_2_dense_3_acc: 0.9859\n",
      "Epoch 3/10\n",
      "22053/22053 [==============================] - 2s 94us/step - loss: 0.0145 - branch_1_dense_3_loss: 0.0000e+00 - branch_2_dense_3_loss: 0.0145 - branch_1_dense_3_acc: 0.2097 - branch_2_dense_3_acc: 0.9951 - val_loss: 0.0557 - val_branch_1_dense_3_loss: 0.0000e+00 - val_branch_2_dense_3_loss: 0.0557 - val_branch_1_dense_3_acc: 0.2020 - val_branch_2_dense_3_acc: 0.9861\n",
      "Epoch 4/10\n",
      "22053/22053 [==============================] - 2s 94us/step - loss: 0.0151 - branch_1_dense_3_loss: 0.0000e+00 - branch_2_dense_3_loss: 0.0151 - branch_1_dense_3_acc: 0.2111 - branch_2_dense_3_acc: 0.9951 - val_loss: 0.0436 - val_branch_1_dense_3_loss: 0.0000e+00 - val_branch_2_dense_3_loss: 0.0436 - val_branch_1_dense_3_acc: 0.2020 - val_branch_2_dense_3_acc: 0.9883\n",
      "Epoch 5/10\n",
      "22053/22053 [==============================] - 2s 93us/step - loss: 0.0078 - branch_1_dense_3_loss: 0.0000e+00 - branch_2_dense_3_loss: 0.0078 - branch_1_dense_3_acc: 0.2113 - branch_2_dense_3_acc: 0.9975 - val_loss: 0.0494 - val_branch_1_dense_3_loss: 0.0000e+00 - val_branch_2_dense_3_loss: 0.0494 - val_branch_1_dense_3_acc: 0.2020 - val_branch_2_dense_3_acc: 0.9887\n",
      "Epoch 6/10\n",
      "22053/22053 [==============================] - 2s 93us/step - loss: 0.0060 - branch_1_dense_3_loss: 0.0000e+00 - branch_2_dense_3_loss: 0.0060 - branch_1_dense_3_acc: 0.2098 - branch_2_dense_3_acc: 0.9975 - val_loss: 0.0430 - val_branch_1_dense_3_loss: 0.0000e+00 - val_branch_2_dense_3_loss: 0.0430 - val_branch_1_dense_3_acc: 0.2020 - val_branch_2_dense_3_acc: 0.9907\n",
      "Epoch 7/10\n",
      "22053/22053 [==============================] - 2s 94us/step - loss: 0.0062 - branch_1_dense_3_loss: 0.0000e+00 - branch_2_dense_3_loss: 0.0062 - branch_1_dense_3_acc: 0.2097 - branch_2_dense_3_acc: 0.9975 - val_loss: 0.0624 - val_branch_1_dense_3_loss: 0.0000e+00 - val_branch_2_dense_3_loss: 0.0624 - val_branch_1_dense_3_acc: 0.2020 - val_branch_2_dense_3_acc: 0.9869\n",
      "Epoch 8/10\n",
      "22053/22053 [==============================] - 2s 95us/step - loss: 0.0063 - branch_1_dense_3_loss: 0.0000e+00 - branch_2_dense_3_loss: 0.0063 - branch_1_dense_3_acc: 0.2092 - branch_2_dense_3_acc: 0.9977 - val_loss: 0.0561 - val_branch_1_dense_3_loss: 0.0000e+00 - val_branch_2_dense_3_loss: 0.0561 - val_branch_1_dense_3_acc: 0.2020 - val_branch_2_dense_3_acc: 0.9894\n",
      "Epoch 9/10\n",
      "22053/22053 [==============================] - 2s 93us/step - loss: 0.0065 - branch_1_dense_3_loss: 0.0000e+00 - branch_2_dense_3_loss: 0.0065 - branch_1_dense_3_acc: 0.2102 - branch_2_dense_3_acc: 0.9979 - val_loss: 0.0590 - val_branch_1_dense_3_loss: 0.0000e+00 - val_branch_2_dense_3_loss: 0.0590 - val_branch_1_dense_3_acc: 0.2020 - val_branch_2_dense_3_acc: 0.9898\n",
      "Epoch 10/10\n",
      "22053/22053 [==============================] - 2s 94us/step - loss: 0.0058 - branch_1_dense_3_loss: 0.0000e+00 - branch_2_dense_3_loss: 0.0058 - branch_1_dense_3_acc: 0.2096 - branch_2_dense_3_acc: 0.9981 - val_loss: 0.0480 - val_branch_1_dense_3_loss: 0.0000e+00 - val_branch_2_dense_3_loss: 0.0480 - val_branch_1_dense_3_acc: 0.2020 - val_branch_2_dense_3_acc: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14407f84388>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model\n",
    "new_model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=Adam(), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# train the model\n",
    "new_model.fit(\n",
    "    X_train_gt, \n",
    "    y={'branch_1_dense_3': y_train_lt_zeros, 'branch_2_dense_3': y_train_gt}, \n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    validation_split=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing on 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss',\n",
       " 'branch_1_dense_3_loss',\n",
       " 'branch_2_dense_3_loss',\n",
       " 'branch_1_dense_3_acc',\n",
       " 'branch_2_dense_3_acc']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve which metrics are returned by evaluate\n",
    "new_model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5139/5139 [==============================] - 1s 105us/step\n",
      "Total loss: 0.013370477953510325\n",
      "Branch 1 accuracy: 0.9961081922553026\n",
      "Branch 2 accuracy: 0.15547771937746463\n"
     ]
    }
   ],
   "source": [
    "# test branch 1 with zeros in branch 2\n",
    "score = new_model.evaluate(X_test_lt, y={'branch_1_dense_3': y_test_lt, 'branch_2_dense_3': y_test_gt_zeros}, verbose=1)\n",
    "print('Total loss:', score[0])\n",
    "print('Branch 1 accuracy:', score[3])\n",
    "print('Branch 2 accuracy:', score[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4861/4861 [==============================] - 1s 105us/step\n",
      "Total loss: 0.040323474580536936\n",
      "Branch 1 accuracy: 0.19687307146419053\n",
      "Branch 2 accuracy: 0.9917712404854968\n"
     ]
    }
   ],
   "source": [
    "# test branch 2 with zeros in branch 1\n",
    "score = new_model.evaluate(X_test_gt, y={'branch_1_dense_3': y_test_lt_zeros, 'branch_2_dense_3': y_test_gt}, verbose=1)\n",
    "print('Total loss:', score[0])\n",
    "print('Branch 1 accuracy:', score[3])\n",
    "print('Branch 2 accuracy:', score[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 105us/step\n",
      "Total loss: 0.0264723296154757\n",
      "Branch 1 accuracy: 0.6076\n",
      "Branch 2 accuracy: 0.562\n"
     ]
    }
   ],
   "source": [
    "# test both branches with mixed zeros\n",
    "score = new_model.evaluate(X_test, y={'branch_1_dense_3': y_test_lt_dual, 'branch_2_dense_3': y_test_gt_dual}, verbose=1)\n",
    "print('Total loss:', score[0])\n",
    "print('Branch 1 accuracy:', score[3])\n",
    "print('Branch 2 accuracy:', score[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 101us/step\n",
      "Total overall accuracy:  0.935\n"
     ]
    }
   ],
   "source": [
    "# call predict so we can compute the correct output based on probabilities\n",
    "pred = new_model.predict(X_test, verbose=1)\n",
    "# concat. two probability arrays into one and extract largest probability\n",
    "pred = np.concatenate((pred[0], pred[1]), axis=1)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "# convert predicition to match label array and compute overall accuracy\n",
    "pred = keras.utils.to_categorical(pred, total_classes)\n",
    "acc = np.sum(np.all(np.equal(pred, y_test), axis=1))/len(pred)\n",
    "print('Total overall accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from comparing the first iteration model to the second iteration branched model that there is great improvement. Even though the first iteration only accepts a five digit set at a time and was not trained on the 5-9 set it surprisingly has a 20% accuracy on the set. However, the average accuracy shows that its marginally better than the sequential model (first iteration) at predicition the full set.\n",
    "\n",
    "After branching and training the new branch on the new set using the same feature extraction we see that each branch individually is capable of predicting their respective sets with 99% accuracy, as we might expect. \n",
    "\n",
    "We note that the branches are operating with complete independence and as a result when passing the dual set (where the labels contain zeros when the digit belongs to the other set) we get each branch's accuracy to fall to 56% due to the complimentary branch predicting a number but our label set verifying with an empty element.\n",
    "\n",
    "For a more accurate measurement we instead use the predict function on the full test set and then compute which element of both branches contains the highest probability and compare it to the labels. These results show the final branched model to have a 92% accuracy of the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these model structure we can implement a continuous learning model that utilizes transfer learning techniques to avoid having to retrain the feature extraction component of the models. \n",
    "\n",
    "Our results show that the model with an independent branch for each new training set is better at predicting in this instance, however the results it outputs may be conflicting especially the more sets that are added on with each new retrain. \n",
    "\n",
    "Alternatively, the sequential model avoids this conflict by maintaining a singular structure during transfers but does so at a cost of underfitting new sets or forgetting old sets. It might be possible to avoid forgetting by writing a custom loss function for the specific training data set used which avoids penalizing and adjusting the weights corresponding to the previous training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
